{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#!{sys.executable} -m pip install --user netCDF\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import matplotlib.animation as animation\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import imageio\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "PIL = transforms.ToPILImage()\n",
    "TTen = transforms.ToTensor()\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "um_64 9870\n",
      "data_128 9870\n",
      "rot_um_92 9870\n",
      "data_64 9870\n",
      "scale_128 9870\n",
      "scale_rot_um_182 9870\n",
      "rot_92 9870\n",
      "rot_um_mag_92 9870\n",
      "mag_64 9870\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data\"\n",
    "\n",
    "for each_dir in os.listdir(data_dir):\n",
    "    print(each_dir, len(os.listdir(os.path.join(data_dir, each_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate ============================================================================\n",
    "def normalize(tensor):\n",
    "    return (tensor - torch.min(tensor))/(torch.max(tensor) - torch.min(tensor))\n",
    "\n",
    "def rotate(img, degree):\n",
    "    #img shape 2*128*128\n",
    "    #2*2 2*1*128*128 -> 2*1*128*128\n",
    "    theta = torch.tensor(degree/180*np.pi)\n",
    "    rot_m = torch.tensor([[torch.cos(theta), -torch.sin(theta)], [torch.sin(theta), torch.cos(theta)]])\n",
    "    img = torch.einsum(\"ab, bcde -> acde\",(rot_m, img.unsqueeze(1))).squeeze(1)\n",
    "    mmin = torch.min(img)\n",
    "    mmax = torch.max(img)\n",
    "    img = normalize(img).data.numpy()\n",
    "    x = TTen(TF.rotate(Image.fromarray(np.uint8(img[0]*255)), degree, expand=True, fill=(0,)))\n",
    "    y = TTen(TF.rotate(Image.fromarray(np.uint8(img[1]*255)), degree, expand=True, fill=(0,)))\n",
    "    rot_img = torch.cat([x, y], dim = 0)\n",
    "    rot_img[rot_img!=0] = normalize(rot_img[rot_img!=0])\n",
    "    rot_img[rot_img!=0] = rot_img[rot_img!=0]*(mmax - mmin) + mmin\n",
    "    return rot_img\n",
    "\n",
    "def pad_after_rot(rot_um_img, target_dim=92):\n",
    "    dim_diff = (target_dim - rot_um_img.shape[-1]) // 2\n",
    "    pad_to_dim = (dim_diff, dim_diff, dim_diff, dim_diff)\n",
    "    return F.pad(rot_um_img, pad_to_dim, 'constant', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UM ===============================================================================\n",
    "def sample_within_spherical():\n",
    "    t = 2 * np.pi * np.random.uniform()\n",
    "    u = np.random.uniform() + np.random.uniform()\n",
    "    r = 2 - u if u > 1 else u\n",
    "    return [r * np.cos(t), r * np.sin(t)]\n",
    "\n",
    "def sample_n_within_spherical(n=1):\n",
    "    return np.array([sample_within_spherical() for i in range(n)]).transpose() \n",
    "\n",
    "def uniform_motion(img, unit_vector):\n",
    "    return img + torch.FloatTensor(unit_vector).repeat(img.shape[0], img.shape[-1]**2).view(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mag ============================================================================\n",
    "def magnitude(img, scalar):\n",
    "    return img * scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale ================================================================================\n",
    "def scale(img, factor):\n",
    "    scale_img = F.interpolate(img.unsqueeze(0), scale_factor = (1, factor, factor), mode=\"trilinear\", align_corners = True).squeeze(0)\n",
    "    return scale_img*(img.max() - img.min())/(scale_img.max() - scale_img.min())/factor\n",
    "\n",
    "def pad_after_scale(scale_im, target_dim=128):\n",
    "    dim_diff = target_dim - scale_im.shape[-1]\n",
    "    padding_left = padding_top = dim_diff // 2\n",
    "    padding_right = padding_bottom = dim_diff - dim_diff // 2\n",
    "    paddings = (padding_left,padding_right, padding_top, padding_bottom)\n",
    "    return F.pad(scale_im, paddings, 'constant', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860aea34869e40b28e108e6b7b718bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9870.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mag_64\n",
    "direc = \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/data_64/sample_\"\n",
    "for i in tqdm(range(9870)):\n",
    "    mag_scalar = 2 - np.random.uniform(0, 2) # ensure 0 is not choosen\n",
    "    img = torch.load(direc + str(i) + \".pt\")#+(torch.rand(1, 2, 1, 1)*4-2\n",
    "    mag_img = magnitude(img, mag_scalar)\n",
    "    #break\n",
    "    torch.save(mag_img, \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/mag_64/sample_\" + str(i) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a52d3a47744a6ea25a30a7718c8af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9870.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# um_64\n",
    "direc = \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/data_64/sample_\"\n",
    "for i in tqdm(range(9870)):\n",
    "    um_vector = sample_n_within_spherical()\n",
    "    img = torch.load(direc + str(i) + \".pt\")\n",
    "    um_img = uniform_motion(img, um_vector)\n",
    "    torch.save(um_img, \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/um_64/sample_\" + str(i) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rot_92\n",
    "direc = \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/data_64/sample_\"\n",
    "for i in range(9870):\n",
    "    degree = (15*(i-7000))%360\n",
    "    img = torch.load(direc + str(i) + \".pt\")#+(torch.rand(1, 2, 1, 1)*4-2\n",
    "    rot_img = torch.cat([rotate(img[j], degree).unsqueeze(0) for j in range(img.shape[0])], dim = 0)\n",
    "    rot_img_padded = pad_after_rot(rot_img)\n",
    "    #break\n",
    "    torch.save(rot_img_padded, \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/rot_92/sample_\" + str(i) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mag_64\n",
    "direc = \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/data_64/sample_\"\n",
    "for i in tqdm(range(9870)):\n",
    "    mag_scalar = 2 - np.random.uniform(0, 2) # ensure 0 is not choosen\n",
    "    img = torch.load(direc + str(i) + \".pt\")#+(torch.rand(1, 2, 1, 1)*4-2\n",
    "    mag_img = magnitude(img, mag_scalar)\n",
    "    #break\n",
    "    torch.save(mag_img, \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/mag_64/sample_\" + str(i) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_128\n",
    "direc = \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/data_64/sample_\"\n",
    "for i in tqdm(range(9870)):\n",
    "    factor = np.random.uniform(0.2, 2)\n",
    "    img = torch.load(direc + str(i) + \".pt\")\n",
    "    scale_img = scale(img, factor)\n",
    "    torch.save(scale_img, \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/scale_64/sample_\" + str(i) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b8809f07bd4da8a3aee441620ee00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9870.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# rot_um_92\n",
    "direc = \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/rot_92/sample_\"\n",
    "for i in tqdm(range(9870)):\n",
    "    um_vector = sample_n_within_spherical()\n",
    "    rot_img = torch.load(direc + str(i) + \".pt\")\n",
    "    rot_um_img = uniform_motion(rot_img, um_vector)\n",
    "    torch.save(rot_um_img, \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/rot_um_92/sample_\" + str(i) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5239fd782f435e83ac9812fa40d9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3057.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# rot_um_mag_92\n",
    "direc = \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/rot_um_92/sample_\"\n",
    "for i in tqdm(range(9870)):\n",
    "    mag_scalar = 2 - np.random.uniform(0, 2) # ensure 0 is not choosen\n",
    "    img = torch.load(direc + str(i) + \".pt\")\n",
    "    rot_um_mag_img = magnitude(img, mag_scalar)\n",
    "    torch.save(rot_um_mag_img, \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/rot_um_mag_92/sample_\" + str(i) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298cf350953c43fd9f85998f33ced6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9870.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# scale_rot_um_182\n",
    "direc = \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/scale_128/sample_\"\n",
    "for i in tqdm(range(9870)):\n",
    "    scale_img = torch.load(direc + str(i) + \".pt\")\n",
    "\n",
    "    # rotate\n",
    "    degree = (15*(i-7000))%360\n",
    "    scale_rot_img = torch.cat([rotate(scale_img[j], degree).unsqueeze(0) for j in range(scale_img.shape[0])], dim = 0)\n",
    "\n",
    "    # UM\n",
    "    um_vector = sample_n_within_spherical()\n",
    "    scale_rot_um_img = uniform_motion(scale_rot_img, um_vector)\n",
    "\n",
    "    # pad\n",
    "    target_dim = math.ceil(64 * 2 * np.sqrt(2)) + 1 # max dimension after scale by 2, and rot by pi/2\n",
    "    padded_scale_rot_um_img = pad_after_scale(scale_rot_um_img, target_dim)\n",
    "    \n",
    "    torch.save(padded_scale_rot_um_img, \"/global/cscratch1/sd/roseyu/Eliza/TF-net/Data/scale_rot_um_182/sample_\" + str(i) + \".pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
